{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"finalData.tsv\", 'r') as fp:\n",
    "\tdata = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "\tdata[i] = data[i].strip('\\n')\n",
    "\tdata[i] = data[i].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "currPoint = []\n",
    "\n",
    "for token in data:\n",
    "\tif token[0] == '':\n",
    "\t\tif len(currPoint) > 0:\n",
    "\t\t\ttweets.append(currPoint)\n",
    "\t\t\tcurrPoint = []\n",
    "\telse:\n",
    "\t\tcurrPoint.append(token)\n",
    "        \n",
    "order = np.random.permutation([i for i in range(len(tweets))])\n",
    "tweets = np.array(tweets)[order].tolist()\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asciiPercentage(s):\n",
    "\tcount = 0.\n",
    "\tfor char in s:\n",
    "\t\tif ord(char) < 128:\n",
    "\t\t\tcount += 1\n",
    "\treturn count/len(s)\n",
    "\n",
    "def vowelPercentage(s):\n",
    "\tvowels = \"aeiou\"\n",
    "\tcount = 0.\n",
    "\tfor char in s:\n",
    "\t\tif char in vowels:\n",
    "\t\t\tcount += 1\n",
    "\treturn count/len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "\n",
    "\t# feature vector\n",
    "\t# word, pos, lang\n",
    "\n",
    "    word = sent[i][0]\n",
    "    wordClean = ''.join([ch for ch in word if ch in 'asdfghjklqwertyuiopzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM']).lower()\n",
    "    normalizedWord = wordClean.lower()\n",
    "    \n",
    "    anyCap = any(char.isupper() for char in word)\n",
    "    allCap = all(char.isupper() for char in word)\n",
    "    hasSpecial = any( ord(char) > 32 and ord(char) < 65 for char in word)\n",
    "    lang = sent[i][1]\n",
    "    \n",
    "    hashTag = word[0] == '#'\n",
    "    mention = word[0] == '@'\n",
    "    \n",
    "    \n",
    "    features = {'word' : word, 'wordClean' : wordClean, 'normalizedWord' : normalizedWord, \\\n",
    "                'lang' : lang,\n",
    "                'isTitle' : word.istitle(), 'wordLength' : len(word), \\\n",
    "                'anyCap' : anyCap, 'allCap' : word.isupper(),\n",
    "                'hasSpecial' : hasSpecial, 'asciiPer' : asciiPercentage(word)}\n",
    "    \n",
    "    \n",
    "    features['suffix5'] = word[-5:]\n",
    "    features['prefix5'] = word[:5]\n",
    "    features['suffix4'] = word[-4:]\n",
    "    features['prefix4'] = word[:4]\n",
    "    features['suffix3'] = word[-3:]\n",
    "    features['prefix3'] = word[:3]\n",
    "    features['suffix2'] = word[-2:]\n",
    "    features['prefix2'] = word[:2]\n",
    "    features['suffix1'] = word[-1:]\n",
    "    features['prefix1'] = word[:1]  \n",
    "    \n",
    "    \n",
    "    if i > 0:\n",
    "\n",
    "        word = sent[i - 1][0]\n",
    "        wordClean = ''.join([ch for ch in word if ch in 'asdfghjklqwertyuiopzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM']).lower()\n",
    "        normalizedWord = wordClean.lower()\n",
    "        lang = sent[i - 1][1]\n",
    "\n",
    "        features['-1:word'] = word\n",
    "        features['-1:wordClean'] = wordClean\n",
    "        features['-1:normalizedWord'] = normalizedWord\n",
    "        features['BOS'] = False\n",
    "        features['-1:lang'] = lang\n",
    "\n",
    "    else:\n",
    "        features['-1:word'] = ''\n",
    "        features['-1:wordClean'] = ''\n",
    "        features['-1:normalizedWord'] = ''\n",
    "        features['BOS'] = True\n",
    "        features['-1:lang'] = ''\n",
    "\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "\n",
    "        word = sent[i + 1][0]\n",
    "        wordClean = ''.join([ch for ch in word if ch in 'asdfghjklqwertyuiopzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM']).lower()\n",
    "        normalizedWord = wordClean.lower()\n",
    "        lang = sent[i + 1][1]\n",
    "\n",
    "        features['+1:word'] = word\n",
    "        features['+1:wordClean'] = wordClean\n",
    "        features['+1:normalizedWord'] = normalizedWord\n",
    "        features['+1:lang'] = lang\n",
    "        features['EOS'] = False\n",
    "        \n",
    "    else:\n",
    "        features['+1:word'] = ''\n",
    "        features['+1:wordClean'] = ''\n",
    "        features['+1:normalizedWord'] = ''\n",
    "        features['+1:lang'] = ''\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "\tfeatures = []\n",
    "\n",
    "\tfor i in range(len(sent)):\n",
    "\t\tfeatures.append(word2features(sent, i))\n",
    "\n",
    "\treturn features\n",
    "\n",
    "def sent2labels(sent):\n",
    "\tallLabels = []\n",
    "\n",
    "\tfor i in sent:\n",
    "\t\tallLabels.append(i[2])\n",
    "\n",
    "\treturn allLabels\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\n",
    "\tallTokens = []\n",
    "\n",
    "\tfor i in sent:\n",
    "\t\tallTokens.append(i[0])\n",
    "\n",
    "\treturn allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Params; obtained from Grid Search\n",
    "\n",
    "c1 = 0.0001\n",
    "c2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation 0 for c1 : 0.0001 c2 : 0.1\n",
      "--> Extracting Train Set ...\n",
      "--> Extracting Test Set ...\n",
      "--> Loading CRF module ...\n",
      "Training ...\n",
      "Testing ...\n",
      "cross validation 1 for c1 : 0.0001 c2 : 0.1\n",
      "--> Extracting Train Set ...\n",
      "--> Extracting Test Set ...\n",
      "--> Loading CRF module ...\n",
      "Training ...\n",
      "Testing ...\n",
      "cross validation 2 for c1 : 0.0001 c2 : 0.1\n",
      "--> Extracting Train Set ...\n",
      "--> Extracting Test Set ...\n",
      "--> Loading CRF module ...\n",
      "Training ...\n",
      "Testing ...\n",
      "cross validation 3 for c1 : 0.0001 c2 : 0.1\n",
      "--> Extracting Train Set ...\n",
      "--> Extracting Test Set ...\n",
      "--> Loading CRF module ...\n",
      "Training ...\n",
      "Testing ...\n",
      "cross validation 4 for c1 : 0.0001 c2 : 0.1\n",
      "--> Extracting Train Set ...\n",
      "--> Extracting Test Set ...\n",
      "--> Loading CRF module ...\n",
      "Training ...\n",
      "Testing ...\n",
      " CRF Classification\n",
      "c1 : 0.0001 c2 : 0.1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ     0.7474    0.7043    0.7252      1542\n",
      "        ADP     0.9350    0.9441    0.9396      2972\n",
      "        ADV     0.8720    0.7760    0.8212      1018\n",
      "       CONJ     0.9585    0.9146    0.9360       808\n",
      "        DET     0.9228    0.8920    0.9071      1139\n",
      "       NOUN     0.8157    0.8604    0.8374      5027\n",
      "        NUM     0.9412    0.9072    0.9239       388\n",
      "       PART     0.8399    0.7169    0.7735      1427\n",
      "   PART_NEG     0.9661    0.9806    0.9733       465\n",
      "       PRON     0.9028    0.8710    0.8866      1450\n",
      "    PRON_WH     0.9577    0.9189    0.9379       419\n",
      "      PROPN     0.9129    0.9199    0.9164      2734\n",
      "       VERB     0.8633    0.9079    0.8850      5959\n",
      "          X     0.9929    0.9833    0.9881      7566\n",
      "\n",
      "avg / total     0.9000    0.8995    0.8992     32914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "chunk = len(tweets) / k\n",
    "results = []\n",
    "\n",
    "allTestPredictions = []\n",
    "allTestGroundTruth = []\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    print \"cross validation\", i, 'for', 'c1 :', c1, 'c2 :', c2\n",
    "\n",
    "    test_sents = tweets[i * chunk : (i + 1) * chunk]\n",
    "    train_sents = tweets[:i * chunk] + tweets[(i + 1) * chunk:]\n",
    "\n",
    "    print \"--> Extracting Train Set ...\"\n",
    "    X_train = [sent2features(s) for s in train_sents]\n",
    "    y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    print \"--> Extracting Test Set ...\"\n",
    "    X_test = [sent2features(s) for s in test_sents]\n",
    "    y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "    print \"--> Loading CRF module ...\"\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': c1,   # coefficient for L1 penalty\n",
    "        'c2': c2,  # coefficient for L2 penalty\n",
    "        'max_iterations': 1000,  # stop earlier\n",
    "\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True,\n",
    "        'feature.possible_states' : True\n",
    "    })\n",
    "\n",
    "    print \"Training ...\"\n",
    "    trainer.train('posPlus_uri_' + str(i))\n",
    "\n",
    "    print \"Testing ...\"\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('posPlus_uri_' + str(i))\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for xseq in X_test:\n",
    "        y_pred.append(tagger.tag(xseq))\n",
    "\n",
    "    \"\"\" CRF based classification \"\"\"\n",
    "\n",
    "    predictedLabels = []\n",
    "    correctLabels = []\n",
    "\n",
    "    for i in y_pred:\n",
    "        for j in i:\n",
    "            predictedLabels.append(j)\n",
    "            allTestPredictions.append(j)\n",
    "\n",
    "    for i in y_test:\n",
    "        for j in i:\n",
    "            correctLabels.append(j)\n",
    "            allTestGroundTruth.append(j)\n",
    "\n",
    "print \"\"\" CRF Classification\"\"\"\n",
    "print 'c1 :', c1, 'c2 :', c2\n",
    "print classification_report(allTestGroundTruth, allTestPredictions, digits = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
